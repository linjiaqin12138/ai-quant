#!/usr/bin/env python3
"""
Web Page Reader Agent
é€šè¿‡Jina APIè¯»å–ç½‘é¡µå†…å®¹ï¼Œå¹¶ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææå–ç”¨æˆ·æŒ‡å®šçš„å†…å®¹è¡ŒèŒƒå›´
"""

import sys
import os

from textwrap import dedent
import traceback
from typing import List, Tuple, Optional
import argparse

import requests

from lib.config import get_http_proxy
from lib.model.error import LlmReplyInvalid
from lib.tools.cache_decorator import use_cache
from lib.utils.decorators import with_retry
from lib.utils.string import extract_json_string

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from lib.modules import get_agent, get_llm_tool
from lib.logger import logger

SYS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µå†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿ä»ç½‘é¡µå†…å®¹ä¸­æå–ç”¨æˆ·éœ€è¦çš„ç‰¹å®šä¿¡æ¯ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æç”¨æˆ·æä¾›çš„ç½‘é¡µå†…å®¹
2. æ ¹æ®ç”¨æˆ·çš„æå–éœ€æ±‚ï¼ˆqueryï¼‰ï¼Œæ‰¾åˆ°ç›¸å…³å†…å®¹åœ¨åŸæ–‡ä¸­çš„è¡ŒèŒƒå›´
3. è¿”å›ç²¾ç¡®çš„è¡Œå·èŒƒå›´ï¼Œæ ¼å¼ä¸º [å¼€å§‹è¡Œå·, ç»“æŸè¡Œå·]

**é‡è¦è§„åˆ™ï¼š**
- è¡Œå·ä»1å¼€å§‹è®¡æ•°
- å¿…é¡»è¿”å›JSONæ ¼å¼ï¼š{"start_line": å¼€å§‹è¡Œå·, "end_line": ç»“æŸè¡Œå·, "reason": "é€‰æ‹©ç†ç”±"}
- å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Œè¿”å›ï¼š{"start_line": -1, "end_line": -1, "reason": "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"}
- é€‰æ‹©èŒƒå›´æ—¶è¦ç¡®ä¿åŒ…å«å®Œæ•´çš„å†…å®¹ï¼Œä¸è¦é—æ¼é‡è¦ä¿¡æ¯
- å¦‚æœç›¸å…³å†…å®¹åˆ†æ•£åœ¨å¤šä¸ªåœ°æ–¹ï¼Œé€‰æ‹©æœ€ä¸»è¦æˆ–æœ€å®Œæ•´çš„éƒ¨åˆ†

**ç¤ºä¾‹ï¼š**
ç”¨æˆ·query: "æå–æ–‡ç« æ­£æ–‡"
åˆ†æï¼šæ–‡ç« æ­£æ–‡é€šå¸¸åœ¨æ ‡é¢˜åï¼Œæ’é™¤å¤´éƒ¨å¯¼èˆªå’Œåº•éƒ¨ä¿¡æ¯
è¿”å›ï¼š{"start_line": 15, "end_line": 89, "reason": "æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œä»æ ‡é¢˜åå¼€å§‹åˆ°å‚è€ƒèµ„æ–™å‰ç»“æŸ"}
"""

def cache_key_generator(kwargs, *args) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    url = kwargs.get('url', '')
    return f"web_page_reader:{url}"

@use_cache(3600, use_db_cache=True, key_generator=cache_key_generator)
@with_retry(
    retry_errors=(ConnectionError, TimeoutError, OSError),
    max_retry_times=3
)
def read_web_page(url: str) -> str:
    """
    ä½¿ç”¨Jina APIè¯»å–ç½‘é¡µå†…å®¹
    
    Args:
        url: è¦è¯»å–çš„ç½‘é¡µURL
    
    Returns:
        ç½‘é¡µå†…å®¹å­—ç¬¦ä¸²
    """
    # Jina Reader APIç«¯ç‚¹
    jina_url = f"https://r.jina.ai/{url}"
    
    # è·å–ä»£ç†è®¾ç½®
    proxy = get_http_proxy()
    proxies = None
    if proxy:
        proxies = {
            'http': proxy,
            'https': proxy
        }
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # å‘é€è¯·æ±‚åˆ°Jina API
    response = requests.get(jina_url, headers=headers, proxies=proxies, timeout=600)
    response.raise_for_status()
    
    # è¿”å›ç½‘é¡µå†…å®¹
    return response.text

class WebPageReader:
    """ç½‘é¡µå†…å®¹è¯»å–å’Œæ™ºèƒ½æå–å™¨"""
    
    def __init__(self, provider: str = "paoluz", model: str = "deepseek-v3"):
        """
        åˆå§‹åŒ–ç½‘é¡µé˜…è¯»å™¨
        
        Args:
            provider: LLMæä¾›å•†
            model: ä½¿ç”¨çš„æ¨¡å‹
        """
        self.provider = provider
        self.model = model
        self.llm_ask = get_llm_tool(
            SYS_PROMPT, 
            provider, 
            model, 
            temperature=0.1,
            response_format='json_object'
        )

    @with_retry((LlmReplyInvalid,), max_retry_times=1)
    def analyze_content_range(self, content: str, query: str) -> Tuple[int, int]:
        """
        åˆ†æå†…å®¹å¹¶è¿”å›æŒ‡å®šqueryå¯¹åº”çš„è¡ŒèŒƒå›´
        
        Args:
            content: ç½‘é¡µå†…å®¹
            query: ç”¨æˆ·çš„æå–éœ€æ±‚
            
        Returns:
            (å¼€å§‹è¡Œå·, ç»“æŸè¡Œå·)
        """
        # ä¸ºå†…å®¹æ·»åŠ è¡Œå·
        lines = content.split('\n')
        numbered_content = []
        for i, line in enumerate(lines, 1):
            numbered_content.append(f"{i:4d}: {line}")
        
        numbered_text = '\n'.join(numbered_content)
        
        # æ„å»ºåˆ†ææç¤º
        prompt = dedent(
            f"""
                è¯·åˆ†æä»¥ä¸‹å¸¦è¡Œå·çš„ç½‘é¡µå†…å®¹ï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚æå–ç›¸åº”çš„è¡ŒèŒƒå›´ã€‚

                ç”¨æˆ·éœ€æ±‚ï¼š{query}

                ç½‘é¡µå†…å®¹ï¼š
                {numbered_text}

                è¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼ŒåŒ…å«start_lineã€end_lineã€‚
            """
        )
        
        # è°ƒç”¨Agentåˆ†æ
        response = self.llm_ask(prompt)
        logger.debug(f"LLMåˆ†æå“åº”: {response}")
        result = extract_json_string(response)
        if not result or "start_line" not in result or "end_line" not in result:
            raise LlmReplyInvalid("LLMè¿”å›çš„ç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘start_lineæˆ–end_lineå­—æ®µ", response)
        start_line = result.get("start_line", -1)
        end_line = result.get("end_line", -1)
        if start_line < 1 or end_line < 1 or start_line > end_line:
            raise LlmReplyInvalid("LLMè¿”å›çš„è¡Œå·ä¸åˆæ³•", response)

        return start_line, end_line

    
    def extract_content_by_range(self, content: str, start_line: int, end_line: int) -> str:
        """
        æ ¹æ®è¡ŒèŒƒå›´æå–å†…å®¹
        
        Args:
            content: åŸå§‹å†…å®¹
            start_line: å¼€å§‹è¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
            end_line: ç»“æŸè¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
            
        Returns:
            æå–çš„å†…å®¹
        """
        if start_line <= 0 or end_line <= 0:
            return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
        
        lines = content.split('\n')
        
        # è½¬æ¢ä¸º0-basedç´¢å¼•å¹¶ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
        start_idx = max(0, start_line - 1)
        end_idx = min(len(lines), end_line)
        
        if start_idx >= len(lines):
            return "è¡Œå·è¶…å‡ºèŒƒå›´"
        
        extracted_lines = lines[start_idx:end_idx]
        return '\n'.join(extracted_lines)
    
    def read_and_extract(self, url: str, query: str) -> str:
        """
        è¯»å–ç½‘é¡µå¹¶æå–æŒ‡å®šå†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            query: æå–éœ€æ±‚, å¦‚ "æå–æ­£æ–‡"ã€"æå–æ‘˜è¦"ç­‰
            
        Returns:
            åŒ…å«æå–ç»“æœçš„å­—å…¸
        """
        
        try:
            # è¯»å–ç½‘é¡µå†…å®¹
            logger.info(f"ğŸ“– æ­£åœ¨è¯»å–ç½‘é¡µ: {url}")
            full_content = read_web_page(url)

            if not full_content.strip():
                return "ç½‘é¡µå†…å®¹ä¸ºç©º"
            
            logger.info(f"âœ… ç½‘é¡µè¯»å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦")
            
            # åˆ†æå†…å®¹èŒƒå›´
            logger.info(f"ğŸ” æ­£åœ¨åˆ†æå†…å®¹ï¼ŒæŸ¥æ‰¾: {query}")
            start_line, end_line = self.analyze_content_range(full_content, query)
            
            # æå–æŒ‡å®šèŒƒå›´çš„å†…å®¹
            extracted_content = self.extract_content_by_range(full_content, start_line, end_line)
            logger.info(f"âœ… æå–æˆåŠŸï¼Œè¡ŒèŒƒå›´: [{start_line}, {end_line}]")
            return extracted_content
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            logger.debug(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return error_msg